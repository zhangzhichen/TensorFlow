1.SPI扩展技术:满足目录名一致，文件名一致，key存在，识别并加载配置的类信息
2.ConditionalOnBean 条件加载

3.zookeeper
zk真正的用意->分布式协调组件
google Chubby 解决分布式一致性问题，分布式锁。   一个服务进去创建文件，其它无法抢占这个文件  zookeeper和这个服务一样的功能
拜占庭将军问题:定义一种协议（paxos）,一致性算法。

GFS->google分布式文件处理系统 存在多个服务，选举出一个master

create lock(唯一索引)
排队进行

集群模式：
master处理事务请求（增删改），同步数据给slave结点
slave处理读请求,接收主结点的同步数据
master挂掉之后，通过选举算法选出新的master:zab协议
(分布式理论：深入浅出Paxos算法)https://my.oschina.net/u/150175/blog/2992187
简述如下:
1.当集群中，超过半数的Acceptor接受了一个议案，那我们就可以说这个议案被选定了（Chosen）。
2.一次选举必须要选定一个议案（不能出现所有议案都被拒绝的情况）。
3.一次选举必须只选定一个议案（不能出现两个议案有不同的值，却都被选定的情况）。

数据同步一致性:
协议->2pc协议：发起请求，如果返回成功，就提交数据。（zk里面，只要集群有过半是返回成功的，就会提交）

leader/follower/observer (observer不参与数据同步，但是需要满足最终数据一致性)

zookeeper的基本特性
1.创建节点 -create
  1)顺序节点
  2)统一级别的节点不能存在重复
  3)创建节点时，必须要带上全路径
  4)临时节点的特性:owner是记录当前创建的id。 然后服务关闭，节点会在心跳检测结束之后删除。
2.删除节点
  1)节点只能一层一层删除
  2)deleteAll可以递归删除
  
watcher
事件被监听后，事件失效。

在java中实现对zookeeper的访问
curator:提供了基本的对zookeeper的增删改查的应用

zookeeper的一致性
Sequential Consistency 顺序一致性(按照时间序，得到的数据一致)

zk两种实现方式:队列，zxid

leader选举模式:
1.server启动时发生选举,每个server都有myId,id最大的节点连接到小的节点
2.每个服务器发起投票，（myid/zxid/epoch(默认是1)）,epoch->leader选举的变化，每一轮选举epoch都会变化
3.先比较epoch,zxid最大的作为leader
4.如果zxid相同，myid最大是leader

整体的流程：
1.一个服务组装一个对象加入sendqueue队列。
2.通过socket建立连接，进行数据传输。
3.通过队列receiveQueue接受消息。

1) 数据恢复阶段
首先,每个在zookeeper服务器先读取当前保存在磁盘的数据,zookeeper中的每份数据,都有一个对应的id值,这个值是依次递增的,换言之,越新的数据,对应的ID值就越大.
2) 向其他节点发送投票值
在读取数据完毕之后,每个zookeeper服务器发送自己选举的leader（首次选自己）,这个协议中包含了以下几部分的数据:
    a)所选举leader的id(就是配置文件中写好的每个服务器的id) ,在初始阶段,每台服务器的这个值都是自己服务器的id,也就是它们都选举自己为leader.
    b) 服务器最大数据的id,这个值大的服务器,说明存放了更新的数据.
    c)逻辑时钟的值,这个值从0开始递增,每次选举对应一个值,也就是说:  如果在同一次选举中,那么这个值应该是一致的 ;  逻辑时钟值越大,说明这一次选举leader的进程更新.
    d) 本机在当前选举过程中的状态,有以下几种:LOOKING,FOLLOWING,OBSERVING,LEADING,顾名思义不必解释了吧.
3）接受来自其他节点的数据
每台服务器将自己服务器的以上数据发送到集群中的其他服务器之后,同样的也需要接收来自其他服务器的数据,它将做以下的处理:
（1）如果所接收数据中服务器的状态还是在选举阶段(LOOKING 状态),那么首先判断逻辑时钟值,又分为以下三种情况:
     a) 如果发送过来的逻辑时钟大于目前的逻辑时钟,那么说明这是更新的一次选举,此时需要更新一下本机的逻辑时钟值,同时将之前收集到的来自其他服务器的选举清空,因为这些数据已经不再有效了.然后判断是否需要更新当前自己的选举情况.在这里是根据选举leader id,保存的最大数据id来进行判断的,这两种数据之间对这个选举结果的影响的权重关系是:首先看数据id,数据id大者胜出;其次再判断leader id,leader id大者胜出.然后再将自身最新的选举结果(也就是上面提到的三种数据）广播给其他服务器).
    b) 发送过来数据的逻辑时钟小于本机的逻辑时钟，说明对方在一个相对较早的选举进程中,这里只需要将本机的数据发送过去就是了
    c) 两边的逻辑时钟相同,此时也只是调用totalOrderPredicate函数判断是否需要更新本机的数据,如果更新了再将自己最新的选举结果广播出去就是了.
然后再处理两种情况:
    1)服务器判断是不是已经收集到了所有服务器的选举状态,如果是，那么这台服务器选举的leader就定下来了，然后根据选举结果设置自己的角色(FOLLOWING还是LEADER),然后退出选举过程就是了.
    2)即使没有收集到所有服务器的选举状态,也可以根据该节点上选择的最新的leader是不是得到了超过半数以上服务器的支持,如果是,那么当前线程将被阻塞等待一段时间(这个时间在finalizeWait定义)看看是不是还会收到当前leader的数据更优的leader,如果经过一段时间还没有这个新的leader提出来，那么这台服务器最终的leader就确定了,否则进行下一次选举. 
（2) 如果所接收服务器不在选举状态,也就是在FOLLOWING或者LEADING状态
做以下两个判断:
    a) 如果逻辑时钟相同,将该数据保存到recvset,如果所接收服务器宣称自己是leader,那么将判断是不是有半数以上的服务器选举它,如果是则设置选举状态退出选举过程
    b) 否则这是一条与当前逻辑时钟不符合的消息,那么说明在另一个选举过程中已经有了选举结果,于是将该选举结果加入到outofelection集合中,再根据outofelection来判断是否可以结束选举,如果可以也是保存逻辑时钟,设置选举状态,退出选举过程.
	

以一个简单的例子来说明整个选举的过程.
假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.
1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态
2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态.
3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.
4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.
5) 服务器5启动,同4一样,当小弟.


watcher机制:
配置中心或者注册中心的核心（通知机制）
三种注册监听的机制 getData()/exists/getChildren 
1.watcher只会监听到一次，就算是watcher事件丢失。


Zookeeper中的原子广播
Zookeeper中的原子广播为了保证节点数据的一致性使用2PC算法并对其进行改进
1.leader接收到请求之后，先将请求记录到本地的日志文件log.xxx中，日志文件的位置由dataDir来决定
2.如果成功，leader节点就会将这些请求放入队列中，然后将队列发送给每一个follower
3.follower接收到请求后，会将这些请求从队列中依次取出，再写到本地的日志文件中
4.如果记录成功，则follower会给leader返回一个成功的信号
5.如果记录失败，则follower会给leader返回一个失败的信号
6.如果leader收到一半及以上follower返回成功信号，那么就会执行，就会要求所有的follower执行这个请求。
7.如果leader没有收到一半follower返回成功信号，那么leader就会认为这个请求不能执行，就会要求所有的follower删除对应的记录。





















